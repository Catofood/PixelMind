{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Catofood/PixelMind/blob/Linear-and-NN-structure/Linear_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "w4NfcW8Ih1La"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# ---------- Функции активации ----------\n",
        "def ReLU(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x: np.array):\n",
        "    # Вычитаем максимум\n",
        "    # (дополнительно можно ограничить значения)\n",
        "    x_max = np.max(x, axis=1, keepdims=True)\n",
        "    x_stable = x - x_max\n",
        "    # Можно добавить огран. например: x_stable = np.clip(x_stable, -15, 15)\n",
        "\n",
        "    exps = np.exp(x_stable)\n",
        "    sums = np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "    # sums может быть inf\n",
        "    # Чтобы избежать деления на 0:\n",
        "    sums = np.clip(sums, 1e-9, None)\n",
        "    return exps / sums\n",
        "\n",
        "\n",
        "# ---------- Функции потерь ----------\n",
        "def categorical_cross_entropy_loss(y_pred: np.array, y_true: np.array):\n",
        "    \"\"\"\n",
        "    y_pred: вероятности (после softmax), shape=(batch_size, n_classes)\n",
        "    y_true: one-hot метки, shape=(batch_size, n_classes)\n",
        "\n",
        "    Возвращает (loss_value, grad),\n",
        "      где grad = dL/dy_pred той же формы, что y_pred.\n",
        "    \"\"\"\n",
        "    eps = 1e-7\n",
        "    # 1) Считаем кросс-энтропию\n",
        "    N = y_pred.shape[0]\n",
        "    loss_value = - np.mean(np.sum(y_true * np.log(y_pred + eps), axis=1))\n",
        "\n",
        "    # 2) Градиент по y_pred\n",
        "    #    dL/dy_pred = -1/N * (y_true / y_pred)\n",
        "    grad = - (y_true / (y_pred + eps)) / N\n",
        "    return loss_value, grad\n",
        "\n",
        "\n",
        "def mse_loss(y_pred, y_true):\n",
        "    N = y_pred.shape[0]\n",
        "    loss_value = np.mean((y_pred - y_true)**2)\n",
        "    grad = 2.0 * (y_pred - y_true) / N  # dL/dy_pred\n",
        "    return loss_value, grad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Производные активаций --\n",
        "def d_relu(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def d_sigmoid(z):\n",
        "    sig = 1 / (1 + np.exp(-z))\n",
        "    return sig * (1 - sig)"
      ],
      "metadata": {
        "id": "jT__smnE6XO8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "    def __init__(self, input_size, output_size, activation_func=None, activation_deriv=None):\n",
        "        self.n_in = input_size\n",
        "        self.n_out = output_size\n",
        "\n",
        "        # Инициализация весов\n",
        "        self.weights = np.random.randn(self.n_in, self.n_out) * 0.01\n",
        "        self.bias = np.zeros((1, self.n_out))  # иногда лучше инициализировать нулями\n",
        "\n",
        "        self.activation_func = activation_func if activation_func else lambda x: x\n",
        "        self.activation_deriv = activation_deriv\n",
        "\n",
        "        # Для backward сохраняем промежуточное\n",
        "        self.input = None\n",
        "        self.z = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x: np.array):\n",
        "        \"\"\"\n",
        "        x: (batch_size, n_in)\n",
        "        \"\"\"\n",
        "        self.input = x\n",
        "        # Линейная часть\n",
        "        self.z = np.dot(x, self.weights) + self.bias\n",
        "        # Активация\n",
        "        self.output = self.activation_func(self.z)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, d_out: np.array, learning_rate: float):\n",
        "        \"\"\"\n",
        "        d_out = dL/d(output этого слоя).\n",
        "\n",
        "        Возвращаем dL/d(input этого слоя).\n",
        "        \"\"\"\n",
        "        # dZ = d_out * activation'(z) (покомпонентное)\n",
        "        if self.activation_deriv is not None:\n",
        "            dZ = d_out * self.activation_deriv(self.z)\n",
        "        else:\n",
        "            dZ = d_out\n",
        "\n",
        "        # dW = X^T * dZ\n",
        "        dW = np.dot(self.input.T, dZ)\n",
        "        # db = сумма dZ по batch\n",
        "        db = np.sum(dZ, axis=0, keepdims=True)\n",
        "        # dX = dZ * W^T\n",
        "        dX = np.dot(dZ, self.weights.T)\n",
        "\n",
        "        # Обновляем параметры\n",
        "        self.weights -= learning_rate * dW\n",
        "        self.bias -= learning_rate * db\n",
        "\n",
        "        return dX"
      ],
      "metadata": {
        "id": "hLcCfgFekNFH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layers=None, loss_func=None, lr=0.01):\n",
        "        self.layers = layers if layers else []\n",
        "        self.loss_func = loss_func  # должна возвращать (loss_value, dL/dy_pred)\n",
        "        self.lr = lr\n",
        "\n",
        "    def add_layer(self, layer: Linear):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x: np.array):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, x: np.array, y_true: np.array):\n",
        "        \"\"\"\n",
        "        1) Forward\n",
        "        2) Считаем loss и dOut\n",
        "        3) backward (reverse layers)\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        y_pred = self.forward(x)\n",
        "        # loss + dL/dy_pred\n",
        "        loss_value, d_out = self.loss_func(y_pred, y_true)\n",
        "\n",
        "        # идём с конца\n",
        "        for layer in reversed(self.layers):\n",
        "            d_out = layer.backward(d_out, self.lr)\n",
        "\n",
        "        return loss_value"
      ],
      "metadata": {
        "id": "DY_ruXEusZOw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======= Пример: бинарная классификация (2 класса) =======\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "\n",
        "    N = 200\n",
        "    d_in = 2\n",
        "    X = np.random.randn(N, d_in)\n",
        "\n",
        "    # Придумаем закономерность: класс 1, если x0 + x1 > 0, иначе класс 0\n",
        "    y_int = np.where(X[:,0] + X[:,1] > 0, 1, 0)\n",
        "\n",
        "    y_true = np.zeros((N, 2))\n",
        "    y_true[np.arange(N), y_int] = 1\n",
        "\n",
        "    # 1) Первый слой (2->8), ReLU\n",
        "    layer1 = Linear(2, 8, activation_func=ReLU, activation_deriv=d_relu)\n",
        "    # 2) Второй слой (8->2), softmax\n",
        "    #    Удобнее softmax сделать отдельным слоем, но сделаем \"в лоб\":\n",
        "    #    linear forward, а activation_func=softmax\n",
        "    #    (Производную softmax напрямую мы не пишем —\n",
        "    #     используем формулу dL/dy_pred=-1/N*y_true/y_pred и предполагаем y_pred=softmax)\n",
        "    layer2 = Linear(8, 2, activation_func=softmax, activation_deriv=None)\n",
        "\n",
        "    # Создаём модель\n",
        "    model = NeuralNetwork(\n",
        "        layers=[layer1, layer2],\n",
        "        loss_func=categorical_cross_entropy_loss,\n",
        "        lr=0.01  # шаг побольше, чтобы обучение шло заметнее\n",
        "    )\n",
        "\n",
        "    # Обучаем несколько эпох\n",
        "    n_epochs = 100\n",
        "    for epoch in range(n_epochs):\n",
        "        loss_val = model.backward(X, y_true)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, loss = {loss_val:.4f}\")\n",
        "\n",
        "    # Проверим результат (предсказанные вероятности)\n",
        "    preds = model.forward(X)  # shape=(N, 2)\n",
        "    # Преобразуем к меткам класса: argmax по оси 1\n",
        "    pred_labels = np.argmax(preds, axis=1)\n",
        "    true_labels = np.argmax(y_true, axis=1)\n",
        "\n",
        "    accuracy = np.mean(pred_labels == true_labels)\n",
        "    print(f\"\\nFinal loss: {loss_val:.4f}, accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "    # Посмотрим на первые 5 примеров\n",
        "    print(\"Some predictions:\")\n",
        "    for i in range(5):\n",
        "        print(f\"X[i]={X[i]},  pred={preds[i]},  pred_class={pred_labels[i]},  true_class={true_labels[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NdFj2_tsDKl",
        "outputId": "bac42711-652a-4039-a02c-32ec8132e9b2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss = 0.6931\n",
            "Epoch 10, loss = 0.6930\n",
            "Epoch 20, loss = 0.6930\n",
            "Epoch 30, loss = 0.6930\n",
            "Epoch 40, loss = 0.6929\n",
            "Epoch 50, loss = 0.6929\n",
            "Epoch 60, loss = 0.6928\n",
            "Epoch 70, loss = 0.6927\n",
            "Epoch 80, loss = 0.6926\n",
            "Epoch 90, loss = 0.6925\n",
            "\n",
            "Final loss: 0.6923, accuracy: 90.50%\n",
            "Some predictions:\n",
            "X[i]=[ 0.49671415 -0.1382643 ],  pred=[0.49983303 0.50016697],  pred_class=1,  true_class=1\n",
            "X[i]=[0.64768854 1.52302986],  pred=[0.49937062 0.50062938],  pred_class=1,  true_class=1\n",
            "X[i]=[-0.23415337 -0.23413696],  pred=[0.50032982 0.49967018],  pred_class=0,  true_class=0\n",
            "X[i]=[1.57921282 0.76743473],  pred=[0.49937905 0.50062095],  pred_class=1,  true_class=1\n",
            "X[i]=[-0.46947439  0.54256004],  pred=[0.50020806 0.49979194],  pred_class=0,  true_class=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def one_hot_encode(y, num_classes):\n",
        "    \"\"\"\n",
        "    Превращает массив целых меток, например [0,1,2,1,0],\n",
        "    в one-hot, например [[1,0,0],[0,1,0],[0,0,1],...].\n",
        "    \"\"\"\n",
        "    y_encoded = np.zeros((y.shape[0], num_classes))\n",
        "    y_encoded[np.arange(y.shape[0]), y] = 1\n",
        "    return y_encoded"
      ],
      "metadata": {
        "id": "h4Zdnlm_eR7W"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDFt-4AiKfr1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}