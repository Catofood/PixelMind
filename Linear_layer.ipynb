{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1735420350032,
     "user": {
      "displayName": "speed066",
      "userId": "15175654710082654690"
     },
     "user_tz": -180
    },
    "id": "w4NfcW8Ih1La"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# ---------- Функции активации ----------\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x: np.array):\n",
    "    x_max = np.max(x, axis=1, keepdims=True)\n",
    "    x_stable = x - x_max\n",
    "    # Дополнительно можно \"подрезать\" слишком большие значения:\n",
    "    x_stable = np.clip(x_stable, -15, 15)\n",
    "    \n",
    "    exps = np.exp(x_stable)\n",
    "    sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    # Также, если sums -> Inf\n",
    "    sums = np.clip(sums, 1e-9, None)\n",
    "    \n",
    "    return exps / sums\n",
    "\n",
    "\n",
    "# ---------- Функции потерь ----------\n",
    "def categorical_cross_entropy_loss(y_pred: np.array, y_true: np.array):\n",
    "    \"\"\"\n",
    "    y_pred: вероятности (после softmax), shape=(batch_size, n_classes)\n",
    "    y_true: one-hot метки, shape=(batch_size, n_classes)\n",
    "\n",
    "    Возвращает (loss_value, grad),\n",
    "      где grad = dL/dy_pred той же формы, что y_pred.\n",
    "    \"\"\"\n",
    "    eps = 1e-7\n",
    "    # 1) Считаем кросс-энтропию\n",
    "    N = y_pred.shape[0]\n",
    "    loss_value = - np.mean(np.sum(y_true * np.log(y_pred + eps), axis=1))\n",
    "\n",
    "    # 2) Градиент по y_pred\n",
    "    #    dL/dy_pred = -1/N * (y_true / y_pred)\n",
    "    grad = - (y_true / (y_pred + eps)) / N\n",
    "    return loss_value, grad\n",
    "\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    N = y_pred.shape[0]\n",
    "    loss_value = np.mean((y_pred - y_true)**2)\n",
    "    grad = 2.0 * (y_pred - y_true) / N  # dL/dy_pred\n",
    "    return loss_value, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 551,
     "status": "ok",
     "timestamp": 1735419917932,
     "user": {
      "displayName": "speed066",
      "userId": "15175654710082654690"
     },
     "user_tz": -180
    },
    "id": "jT__smnE6XO8"
   },
   "outputs": [],
   "source": [
    "# -- Производные активаций --\n",
    "def d_relu(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    sig = 1 / (1 + np.exp(-z))\n",
    "    return sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1735420052541,
     "user": {
      "displayName": "speed066",
      "userId": "15175654710082654690"
     },
     "user_tz": -180
    },
    "id": "hLcCfgFekNFH"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size, activation_func=None, activation_deriv=None):\n",
    "        self.n_in = input_size\n",
    "        self.n_out = output_size\n",
    "\n",
    "        # Инициализация весов\n",
    "        self.weights = np.random.randn(self.n_in, self.n_out) * 0.01\n",
    "        self.bias = np.zeros((1, self.n_out))  # иногда лучше инициализировать нулями\n",
    "\n",
    "        self.activation_func = activation_func if activation_func else lambda x: x\n",
    "        self.activation_deriv = activation_deriv\n",
    "\n",
    "        # Для backward сохраняем промежуточное\n",
    "        self.input = None\n",
    "        self.z = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, x: np.array):\n",
    "        \"\"\"\n",
    "        x: (batch_size, n_in)\n",
    "        \"\"\"\n",
    "        self.input = x\n",
    "        # Линейная часть\n",
    "        self.z = np.dot(x, self.weights) + self.bias\n",
    "        # Активация\n",
    "        self.output = self.activation_func(self.z)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out: np.array, learning_rate: float):\n",
    "        \"\"\"\n",
    "        d_out = dL/d(output этого слоя).\n",
    "\n",
    "        Возвращаем dL/d(input этого слоя).\n",
    "        \"\"\"\n",
    "        # dZ = d_out * activation'(z) (покомпонентное)\n",
    "        if self.activation_deriv is not None:\n",
    "            dZ = d_out * self.activation_deriv(self.z)\n",
    "        else:\n",
    "            dZ = d_out\n",
    "\n",
    "        # dW = X^T * dZ\n",
    "        dW = np.dot(self.input.T, dZ)\n",
    "        # db = сумма dZ по batch\n",
    "        db = np.sum(dZ, axis=0, keepdims=True)\n",
    "        # dX = dZ * W^T\n",
    "        dX = np.dot(dZ, self.weights.T)\n",
    "\n",
    "        # Обновляем параметры\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.bias -= learning_rate * db\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1735420091484,
     "user": {
      "displayName": "speed066",
      "userId": "15175654710082654690"
     },
     "user_tz": -180
    },
    "id": "DY_ruXEusZOw"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers=None, loss_func=None, lr=0.01):\n",
    "        self.layers = layers if layers else []\n",
    "        self.loss_func = loss_func  # должна возвращать (loss_value, dL/dy_pred)\n",
    "        self.lr = lr\n",
    "\n",
    "    def add_layer(self, layer: Linear):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x: np.array):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, x: np.array, y_true: np.array):\n",
    "        \"\"\"\n",
    "        1) Forward\n",
    "        2) Считаем loss и dOut\n",
    "        3) backward (reverse layers)\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        y_pred = self.forward(x)\n",
    "        # loss + dL/dy_pred\n",
    "        loss_value, d_out = self.loss_func(y_pred, y_true)\n",
    "\n",
    "        # идём с конца\n",
    "        for layer in reversed(self.layers):\n",
    "            d_out = layer.backward(d_out, self.lr)\n",
    "\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3709,
     "status": "ok",
     "timestamp": 1735421202570,
     "user": {
      "displayName": "speed066",
      "userId": "15175654710082654690"
     },
     "user_tz": -180
    },
    "id": "h4Zdnlm_eR7W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: (120, 4) (120, 3)\n",
      "Test shapes: (30, 4) (30, 3)\n",
      "\n",
      "Sample X_train[0]: [-1.47393679  1.20365798 -1.56253475 -1.31260281]\n",
      "Sample y_train[0]: [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    \"\"\"\n",
    "    Превращает массив целых меток, например [0,1,2,1,0],\n",
    "    в one-hot, например [[1,0,0],[0,1,0],[0,0,1],...].\n",
    "    \"\"\"\n",
    "    y_encoded = np.zeros((y.shape[0], num_classes))\n",
    "    y_encoded[np.arange(y.shape[0]), y] = 1\n",
    "    return y_encoded\n",
    "\n",
    "def prepare_iris_dataset(test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Возвращает (X_train, y_train), (X_test, y_test) в удобном формате.\n",
    "    - X_train, X_test: numpy-массивы признаков формы (N, 4)\n",
    "    - y_train, y_test: numpy-массивы one-hot меток формы (N, 3)\n",
    "    \"\"\"\n",
    "    # 1) Загружаем\n",
    "    iris = load_iris()\n",
    "    X = iris.data  # shape = (150, 4) - 4 признака\n",
    "    y = iris.target  # shape = (150,) - метки 0..2\n",
    "    \n",
    "    # 2) Делим на обучающую и тестовую выборку\n",
    "    X_train, X_test, y_train_int, y_test_int = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # 3) Переводим метки в one-hot\n",
    "    y_train = one_hot_encode(y_train_int, num_classes=3)\n",
    "    y_test = one_hot_encode(y_test_int, num_classes=3)\n",
    "    \n",
    "    # 4) (Опционально) нормализуем / стандартизируем признаки\n",
    "    # Например, можно сделать простую стандартизацию:\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0) + 1e-9\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    (X_train, y_train), (X_test, y_test) = prepare_iris_dataset()\n",
    "    \n",
    "    print(\"Train shapes:\", X_train.shape, y_train.shape)  # (120, 4), (120, 3)\n",
    "    print(\"Test shapes:\", X_test.shape, y_test.shape)     # (30, 4), (30, 3)\n",
    "    \n",
    "    # Посмотрим часть данных\n",
    "    print(\"\\nSample X_train[0]:\", X_train[0])\n",
    "    print(\"Sample y_train[0]:\", y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 1.0986\n",
      "epoch 10: loss = 1.0985\n",
      "epoch 20: loss = 1.0984\n",
      "epoch 30: loss = 1.0982\n",
      "epoch 40: loss = 1.0979\n",
      "epoch 50: loss = 1.0975\n",
      "epoch 60: loss = 1.0970\n",
      "epoch 70: loss = 1.0960\n",
      "epoch 80: loss = 1.0946\n",
      "epoch 90: loss = 1.0922\n",
      "epoch 100: loss = 1.0886\n",
      "epoch 110: loss = 1.0828\n",
      "epoch 120: loss = 1.0738\n",
      "epoch 130: loss = 1.0601\n",
      "epoch 140: loss = 1.0396\n",
      "epoch 150: loss = 1.0104\n",
      "epoch 160: loss = 0.9707\n",
      "epoch 170: loss = 0.9199\n",
      "epoch 180: loss = 0.8593\n",
      "epoch 190: loss = 0.7921\n",
      "epoch 200: loss = 0.7225\n",
      "epoch 210: loss = 0.6551\n",
      "epoch 220: loss = 0.5939\n",
      "epoch 230: loss = 0.5417\n",
      "epoch 240: loss = 0.5001\n",
      "epoch 250: loss = 0.4689\n",
      "epoch 260: loss = 0.4471\n",
      "epoch 270: loss = 0.4325\n",
      "epoch 280: loss = 0.4226\n",
      "epoch 290: loss = 0.4152\n",
      "epoch 300: loss = 0.4090\n",
      "epoch 310: loss = 0.4036\n",
      "epoch 320: loss = 0.3987\n",
      "epoch 330: loss = 0.3945\n",
      "epoch 340: loss = 0.3905\n",
      "epoch 350: loss = 0.3866\n",
      "epoch 360: loss = 0.3827\n",
      "epoch 370: loss = 0.3786\n",
      "epoch 380: loss = 0.3744\n",
      "epoch 390: loss = 0.3699\n",
      "epoch 400: loss = 0.3653\n",
      "epoch 410: loss = 0.3605\n",
      "epoch 420: loss = 0.3555\n",
      "epoch 430: loss = 0.3504\n",
      "epoch 440: loss = 0.3453\n",
      "Accuracy 80.00\n",
      "---Первые 10 предсказаний и тестов---\n",
      "Pred: [[1.76084071e-07 4.24377937e-01 5.75621887e-01]\n",
      " [9.99999388e-01 3.05902133e-07 3.05902133e-07]\n",
      " [2.74478994e-07 1.02723127e-01 8.97276599e-01]\n",
      " [1.75855357e-07 4.25125606e-01 5.74874218e-01]\n",
      " [2.05026197e-07 3.29765597e-01 6.70234198e-01]\n",
      " [9.99999388e-01 3.05902133e-07 3.05902133e-07]\n",
      " [2.54383905e-07 8.31585404e-01 1.68414342e-01]\n",
      " [2.27901981e-07 2.54984238e-01 7.45015534e-01]\n",
      " [2.15733023e-07 2.94764785e-01 7.05234999e-01]\n",
      " [1.80263522e-07 5.89284584e-01 4.10715236e-01]]\n",
      "Test: [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    Linear(4, 5, ReLU, d_relu),\n",
    "    Linear(5, 3, softmax),\n",
    "]\n",
    "model = NeuralNetwork(layers=layers,loss_func=categorical_cross_entropy_loss, lr=0.01)\n",
    "\n",
    "n_epoch = 450\n",
    "for n in range(n_epoch):\n",
    "    loss_val = model.backward(X_train, y_train)\n",
    "    if n % 10 == 0:\n",
    "        print(f'epoch {n}: loss = {loss_val:.4f}')\n",
    "\n",
    "preds = model.forward(X_test)\n",
    "preds_labels = np.argmax(preds, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(preds_labels == true_labels)\n",
    "print(f'Accuracy {accuracy * 100:.2f}')\n",
    "\n",
    "print('---Первые 10 предсказаний и тестов---')\n",
    "print(f'Pred: {preds[:10]}')\n",
    "print(f'Test: {y_test[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
